{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Algorithms - Regression Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we are trying to predict how much an individual customer will spend during a black Friday sale. This type of modeling is particularly useful for pricing campaigns and creating deals for specific customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Product_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>City_Category</th>\n",
       "      <th>Stay_In_Current_City_Years</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Product_Category_1</th>\n",
       "      <th>Product_Category_2</th>\n",
       "      <th>Product_Category_3</th>\n",
       "      <th>Purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00069042</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00248942</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00087842</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00085442</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000002</td>\n",
       "      <td>P00285442</td>\n",
       "      <td>M</td>\n",
       "      <td>55+</td>\n",
       "      <td>16</td>\n",
       "      <td>C</td>\n",
       "      <td>4+</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID Product_ID Gender   Age  Occupation City_Category   \n",
       "0  1000001  P00069042      F  0-17          10             A  \\\n",
       "1  1000001  P00248942      F  0-17          10             A   \n",
       "2  1000001  P00087842      F  0-17          10             A   \n",
       "3  1000001  P00085442      F  0-17          10             A   \n",
       "4  1000002  P00285442      M   55+          16             C   \n",
       "\n",
       "  Stay_In_Current_City_Years  Marital_Status  Product_Category_1   \n",
       "0                          2               0                   3  \\\n",
       "1                          2               0                   1   \n",
       "2                          2               0                  12   \n",
       "3                          2               0                  12   \n",
       "4                         4+               0                   8   \n",
       "\n",
       "   Product_Category_2  Product_Category_3  Purchase  \n",
       "0                 NaN                 NaN      8370  \n",
       "1                 6.0                14.0     15200  \n",
       "2                 NaN                 NaN      1422  \n",
       "3                14.0                 NaN      1057  \n",
       "4                 NaN                 NaN      7969  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age\n",
       "26-35    219587\n",
       "36-45    110013\n",
       "18-25     99660\n",
       "46-50     45701\n",
       "51-55     38501\n",
       "55+       21504\n",
       "0-17      15102\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Age.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we need to fix a few of the following columns:\n",
    "\n",
    "**Occupation**: convert to categorical\n",
    "\n",
    "**Maritial_status**: convert to categorical\n",
    "\n",
    "**product_categories**: convert to categorical\n",
    "\n",
    "**Stay_In_Current_City_Years**: convert to numeric\n",
    "\n",
    "**Age**: Format the age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to categorical from numeric\n",
    "df['Occupation'] = pd.Categorical(df['Occupation'])\n",
    "df['Marital_Status'] = pd.Categorical(df['Marital_Status'])\n",
    "df['Product_Category_1'] = pd.Categorical(df['Product_Category_1'])\n",
    "df['Product_Category_2'] = pd.Categorical(df['Product_Category_2'])\n",
    "df['Product_Category_3'] = pd.Categorical(df['Product_Category_3'])\n",
    "\n",
    "# Change to numeric\n",
    "if df['Stay_In_Current_City_Years'].dtype == 'object':\n",
    "    df['Stay_In_Current_City_Years'] = df['Stay_In_Current_City_Years'].str.replace('+', '', regex=False).astype(int)\n",
    "\n",
    "# Fix age column\n",
    "def random_age_replace(df_in, old_col, new_col):\n",
    "    \"\"\"\n",
    "    Replaces age ranges with random integers within the specified ranges.\n",
    "    \n",
    "    Parameters:\n",
    "        df_in (pd.DataFrame): Input DataFrame.\n",
    "        old_col (str): Column with age ranges.\n",
    "        new_col (str): New column for random ages.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Modified DataFrame with the new age column.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    df_out = df_in.copy()\n",
    "    df_out[new_col] = df_out[old_col]\n",
    "    \n",
    "    # Iterate over unique values in the age column\n",
    "    for age_range in df_out[old_col].unique():\n",
    "        if '-' in age_range:  # Handle ranges like \"20-30\"\n",
    "            low, high = map(int, age_range.split('-'))\n",
    "            size = df_out[df_out[old_col] == age_range].shape[0]\n",
    "            df_out.loc[df_out[old_col] == age_range, new_col] = np.random.randint(low, high, size)\n",
    "        else:  # Handle single values like \"55+\"\n",
    "            size = df_out[df_out[old_col] == age_range].shape[0]\n",
    "            df_out.loc[df_out[old_col] == age_range, new_col] = np.random.randint(55, 90, size)\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "# Apply the function\n",
    "df = random_age_replace(df, 'Age', 'New_Age')\n",
    "df['New_Age'] = df['New_Age'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploratory Data Analysis**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAGgCAYAAABohimGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAtsklEQVR4nO3de3RV5Z3/8c/ZuSfkaGRMwkhRBg0RuSSBIGkFMaUsRrGriHWWElpRKF5RkIKWVAUqYyUTqVREFiCoMKgFqQ5OpTh1OnYgJtGqY0AMN4GSRAy5ALmQs/fvD3459ZhgQnhONid5v9Zihezn2V+e8z17Jx/O3jnxOI7jCAAAAOfMcnsBAAAAXQXBCgAAwBCCFQAAgCEEKwAAAEMIVgAAAIYQrAAAAAwhWAEAABhCsAIAADAk3O0FhArHcWTbwXkvVcvyBK02zoy+u4O+u4O+u4O+u6O575blkcfj6dR/m2DVTrbtqLLyhPG64eGWEhLiVFNzUk1NtvH6aB19dwd9dwd9dwd9d8fX++71xigsrHODFZcCAQAADCFYAQAAGEKwAgAAMIRgBQAAYAjBCgAAwBCCFQAAgCEEKwAAAEMIVgAAAIYQrAAAAAwhWAEAABhCsAIAADCEYAUAAGAIwQoAAMAQghUAAIAhBCsAAABDwt1eANBZLMsjy/JIksLCrICP5yvbdmTbjtvLAAC0E8EK3YJleXThhbEtgpTXG+PSitrH57NVVXWScAUAIYJghW7BsjwKC7OUt65Yh8pr3V5Ou/ROitfsSUNlWR6CFQCECIIVupVD5bXac7ja7WUAALqo8/sGEwAAgBBCsAIAADCEYAUAAGAIwQoAAMAQghUAAIAhBCsAAABDCFYAAACGEKwAAAAMIVgBAAAYQrACAAAwhGAFAABgCMEKAADAEIIVAACAIQQrAAAAQwhWAAAAhhCsAAAADCFYAQAAGEKwAgAAMIRgBQAAYAjBCgAAwBCCFQAAgCEEKwAAAEMIVgAAAIYQrAAAAAwhWAEAABhyTsHq+eef1+TJkwO2/dd//ZcmTpyo9PR0ZWdn69e//rXq6+v94w0NDZo/f76ysrKUnp6uhx56SJWVlQE1tm/frptuuklDhgzRuHHjtGXLloBxEzUAAABM63CwWrdunZYsWRKwraioSPfdd59+8IMf6PXXX9djjz2mt956S/Pnz/fPefzxx/Xee+9p6dKlWrt2rfbu3asZM2b4x/fs2aPp06dr5MiR2rRpk3784x9rzpw52r59u9EaAAAApoWf7Q7l5eV67LHHVFBQoMsuuyxgbMOGDbr66qt11113SZIuu+wyzZw5U7m5uZo/f76OHTumzZs3a/ny5Ro2bJgkKT8/X+PGjdOHH36o9PR0rV27Vv3799fMmTMlSf369VNJSYlWrlyprKwslZeXn3MNAACAYDjrYPXpp58qIiJCb7zxhp599lkdPnzYP3bHHXfIsgJfBLMsS6dOndLx48dVXFwsSRoxYoR/vG/fvkpKSlJhYaHS09NVVFSkMWPGBNQYMWKEnnjiCTmOY6SGx+M524ctSQoPN39LWliYFfARwRHK/Q3ltX8Tx7s76Ls76Ls73O77WQer7OxsZWdntzo2YMCAgM9PnTqlNWvWaODAgbroootUXl6uhIQERUVFBcxLTExUWVmZJKmsrEzJycktxuvq6nTs2DEjNS666KKzfdiyLI8SEuLOer/28npjglYboa0rHhtd8TGFAvruDvruDrf6ftbBqr2ampo0Z84cff7551q3bp0kqa6uTpGRkS3mRkVFqaGhQZJUX1/fYk7z542NjUZqdIRtO6qpOdmhfb9NWJglrzdGNTV18vls4/VxWnOfQ1FXOjY43t1B391B393x9b7HxUV1+itXQQlWx48f14MPPqj3339fv/3tbzV48GBJUnR0dKvBpqGhQTExp7/pRUVFtZjT/HlMTIyRGh3V1BS8E8Pns4NaH6GrKx4bXfExhQL67g767g63wqzxYFVRUaFp06bp8OHDWrVqlTIzM/1jycnJqqqqUmNjY8ArShUVFUpKSpIk9erVSxUVFS1qxsbGKj4+3kgNAACAYDD6+lh1dbV++tOfqrKyUuvWrQsIVZI0dOhQ2bbtvwFdkvbt26fy8nL/3GHDhun9998P2G/Hjh3KyMiQZVlGagAAAASD0ZTxr//6rzp48KAWL16siy66SF9++aX/j8/nU1JSkm644Qbl5uaqoKBAH3/8sWbNmqXhw4crLS1NkjR58mR9/PHHysvL0549e7R69Wr94Q9/0NSpUyXJSA0AAIBgMHYp0Ofz6a233tKpU6f005/+tMX4O++8o969e2vhwoVatGiR7rvvPknSqFGjlJub6593xRVXaNmyZVq8eLHWrl2r3r17a/HixQHvP2WiBgAAgGkex3EctxcRCnw+W5WVJ4zXDQ+3lJAQp2PHTnBzYxA19/nB/He153C128tpl36XXKAls0Z3qWOD490d9N0d9N0dX++71xvT6T8VyA1HAAAAhhCsAAAADCFYAQAAGEKwAgAAMIRgBQAAYAjBCgAAwBCCFQAAgCEEKwAAAEMIVgAAAIYQrAAAAAwhWAEAABhCsAIAADCEYAUAAGAIwQoAAMAQghUAAIAhBCsAAABDCFYAAACGEKwAAAAMIVgBAAAYQrACAAAwhGAFAABgCMEKAADAEIIVAACAIQQrAAAAQwhWAAAAhhCsAAAADCFYAQAAGEKwAgAAMIRgBQAAYAjBCgAAwBCCFQAAgCEEKwAAAEPC3V4AgG8XFhZa//+xbUe27bi9DABwBcEKOE9dGB8l23bk9ca4vZSz4vPZqqo6SbgC0C0RrIDzVI+YCFmWR3nrinWovNbt5bRL76R4zZ40VJblIVgB6JYIVsB57lB5rfYcrnZ7GQCAdgitmzcAAADOYwQrAAAAQwhWAAAAhhCsAAAADCFYAQAAGHJOwer555/X5MmTA7bt3LlTOTk5SktLU3Z2tl588cWAcdu29cwzz2jkyJFKS0vTtGnTdPDgwU6vAQAAYFqHg9W6deu0ZMmSgG3Hjh3TlClT1KdPH23cuFH33nuv8vLytHHjRv+cZcuWaf369Vq4cKE2bNgg27Y1depUNTY2dmoNAAAA0876fazKy8v12GOPqaCgQJdddlnA2KuvvqqIiAgtWLBA4eHh6tevnw4cOKAVK1Zo4sSJamxs1OrVqzV79myNHj1akvT0009r5MiR2rp1q8aPH98pNQAAAILhrF+x+vTTTxUREaE33nhDQ4YMCRgrKirS8OHDFR7+97w2YsQI7d+/X0ePHtWuXbt04sQJZWVl+ce9Xq8GDBigwsLCTqsBAAAQDGf9ilV2drays7NbHSsrK1NKSkrAtsTEREnSkSNHVFZWJknq1atXiznNY51R4x/+4R/a8UhbCg83f69/8y/YDbVftBtq6G/nOlO/Od7dQd/dQd/d4Xbfjf5Km/r6ekVGRgZsi4qKkiQ1NDSorq5OklqdU11d3Wk1OsKyPEpIiOvQvu0Rar9oF/g2bR3PHO/uoO/uoO/ucKvvRoNVdHS0/wbyZs1BJjY2VtHR0ZKkxsZG/9+b58TExHRajY6wbUc1NSc7tO+3CQuz5PXGqKamTj6fbbw+TmvuMzrHmY5njnd30Hd30Hd3fL3vcXFRnf7KldFglZycrIqKioBtzZ8nJSWpqanJv61Pnz4Bc/r3799pNTqqqSl4J4bPZwe1PtCZ2jqeOd7dQd/dQd/d4VaYNRrjMjMzVVxcLJ/P59+2Y8cO9e3bVz179lRqaqp69OihgoIC/3hNTY1KSkqUmZnZaTUAAACCwWiwmjhxoo4fP6558+aptLRUmzZt0po1azR9+nRJp++LysnJUV5ent555x3t2rVLM2fOVHJyssaOHdtpNQAAAILB6KXAnj17auXKlXriiSc0YcIEXXzxxZozZ44mTJjgnzNjxgw1NTUpNzdX9fX1yszM1KpVqxQREdGpNQAAAEzzOI7juL2IUODz2aqsPGG8bni4pYSEOB07doJr8EHU3OcH89/VnsPVbi+nXa5Nv0Szc4aF1Jr7XXKBlswafcbjmePdHfTdHfTdHV/vu9cb0+k3r/PmGgAAAIYQrAAAAAwhWAEAABhCsAIAADCEYAUAAGAIwQoAAMAQghUAAIAhBCsAAABDCFYAAACGEKwAAAAMIVgBAAAYQrACAAAwhGAFAABgCMEKAADAEIIVAACAIQQrAAAAQwhWAAAAhhCsAAAADCFYAQAAGEKwAgAAMIRgBQAAYAjBCgAAwBCCFQAAgCEEKwAAAEMIVgAAAIYQrAAAAAwhWAEAABhCsAIAADCEYAUAAGAIwQoAAMAQghUAAIAhBCsAAABDCFYAAACGEKwAAAAMIVgBAAAYQrACAAAwhGAFAABgCMEKAADAEIIVAACAIQQrAAAAQwhWAAAAhhgPVk1NTfrNb36j6667Tunp6Zo0aZL++te/+sd37typnJwcpaWlKTs7Wy+++GLA/rZt65lnntHIkSOVlpamadOm6eDBgwFzTNQAAAAwzXiweu655/Taa69p4cKF2rx5s/r27aupU6eqoqJCx44d05QpU9SnTx9t3LhR9957r/Ly8rRx40b//suWLdP69eu1cOFCbdiwQbZta+rUqWpsbJQkIzUAAACCwXiw2rZtm8aPH69rrrlGl156qR5++GHV1tbqr3/9q1599VVFRERowYIF6tevnyZOnKjbb79dK1askCQ1NjZq9erVmjFjhkaPHq3U1FQ9/fTTKisr09atWyXJSA0AAIBgMB6sevbsqT/96U86dOiQfD6fXnnlFUVGRio1NVVFRUUaPny4wsPD/fNHjBih/fv36+jRo9q1a5dOnDihrKws/7jX69WAAQNUWFgoSUZqAAAABEN421POzrx58/TAAw/o+9//vsLCwmRZlpYuXao+ffqorKxMKSkpAfMTExMlSUeOHFFZWZkkqVevXi3mNI+ZqNFR4eHm7/UPC7MCPiI46G/nOlO/Od7dQd/dQd/d4XbfjQer0tJSxcfH69lnn1VSUpJee+01zZ49Wy+//LLq6+sVGRkZMD8qKkqS1NDQoLq6OklqdU51dbUkGanREZblUUJCXIf3b4vXGxO02kBna+t45nh3B313B313h1t9Nxqsjhw5ooceekhr1qzRsGHDJEmDBg1SaWmpli5dqujo6BY3kDc0NEiSYmNjFR0dLen0fVLNf2+eExNzukEmanSEbTuqqTnZ4f3PJCzMktcbo5qaOvl8tvH6OK25z+gcZzqeOd7dQd/dQd/d8fW+x8VFdforV0aD1UcffaRTp05p0KBBAduHDBmiP//5z/rHf/xHVVRUBIw1f56UlKSmpib/tj59+gTM6d+/vyQpOTn5nGt0VFNT8E4Mn88Oan2gM7V1PHO8u4O+u4O+u8OtMGs0xiUnJ0uSPvvss4Dtu3fv1mWXXabMzEwVFxfL5/P5x3bs2KG+ffuqZ8+eSk1NVY8ePVRQUOAfr6mpUUlJiTIzMyXJSA0AAIBgMBqsBg8erKFDh2ru3LnasWOH9u/fryVLlmj79u362c9+pokTJ+r48eOaN2+eSktLtWnTJq1Zs0bTp0+XdPq+qJycHOXl5emdd97Rrl27NHPmTCUnJ2vs2LGSZKQGAABAMBi9FGhZlp577jktWbJEjzzyiKqrq5WSkqI1a9ZoyJAhkqSVK1fqiSee0IQJE3TxxRdrzpw5mjBhgr/GjBkz1NTUpNzcXNXX1yszM1OrVq1SRESEpNNv53CuNQAAAILB4ziO4/YiQoHPZ6uy8oTxuuHhlhIS4nTs2AmuwQdRc58fzH9Xew53/KdDO9O16Zdods6wkFpzv0su0JJZo894PHO8u4O+u4O+u+Prffd6Y0L75nUAkELvfaxs25Ft839MAOeOYAXAmAvjo2TbTsi9j5XPZ6uq6iThCsA5I1gBMKZHTIQsy6O8dcU6VF7r9nLapXdSvGZPGirL8hCsAJwzghUA4w6V14bMfWEAYNL5daMDAABACCNYAQAAGEKwAgAAMIRgBQAAYAjBCgAAwBCCFQAAgCEEKwAAAEMIVgAAAIYQrAAAAAwhWAEAABhCsAIAADCEYAUAAGAIwQoAAMAQghUAAIAhBCsAAABDCFYAAACGEKwAAAAMIVgBAAAYQrACAAAwhGAFAABgCMEKAADAEIIVAACAIQQrAAAAQwhWAAAAhhCsAAAADCFYAQAAGEKwAgAAMIRgBQAAYAjBCgAAwJBwtxcAADh7luWRZXnaNTcszAr46BbbdmTbjqtrAIKNYAUAIcayPLrwwtizDkpeb0yQVtQ+Pp+tqqqThCt0aQQrAAgxluVRWJilvHXFOlRe6/Zy2qV3UrxmTxoqy/IQrNClEawAIEQdKq/VnsPVbi8DwNdw8zoAAIAhBCsAAABDCFYAAACGEKwAAAAMCUqw2rx5s66//noNGjRIN9xwg/7zP//TP3bo0CFNnz5dGRkZuuaaa7RkyRL5fL6A/detW6fvf//7Gjx4sG677TaVlJQEjJuoAQAAYJrxYPX73/9e8+bN06RJk7RlyxaNHz9es2bN0ocffqhTp07pzjvvlCRt2LBBjz/+uP793/9dzz77rH//119/XU899ZQeeOABbdq0Sb1799aUKVNUWVkpSUZqAAAABIPRYOU4jn7zm9/oJz/5iSZNmqQ+ffro7rvv1ne/+129//77evvtt/W3v/1NTz31lFJSUjRmzBjNmjVLa9euVWNjoyRp+fLlysnJ0Q9/+ENdfvnlWrRokWJiYvTaa69JkpEaAAAAwWA0WO3bt0+HDx/WjTfeGLB91apVmj59uoqKinTVVVfpggsu8I+NGDFCx48f186dO/XVV19p//79ysrK8o+Hh4dr2LBhKiwslCQjNQAAAILB6BuE7tu3T5J08uRJ3XnnnSopKVHv3r119913Kzs7W2VlZUpOTg7YJzExUZJ05MgRhYefXk6vXr1azNm1a5ckGanRUeHh5m9JO19+h1dXR3/RllA6RkJprd8Uyms/W3x9d4fbfTcarI4fPy5Jmjt3ru677z7Nnj1bb7/9tu655x698MILqq+vl9frDdgnKipKktTQ0KC6ujpJUmRkZIs5DQ0NkmSkRkdYlkcJCXEd3r8tbv8OL6C74xzsHN2xz93xMZ8P3Oq70WAVEREhSbrzzjs1YcIESdKVV16pkpISvfDCC4qOjvbfB9WsOezExsYqOjpaklqdExNzukEmanSEbTuqqTnZ4f3PJCzMktcbo5qaOvl8tvH6OK25z8CZhNI5GMrHcyj1+Vzx9d0dX+97XFxUp79yZTRYJSUlSZJSUlICtl9++eV69913NXz4cO3evTtgrKKiwr9v8+W7iooK9evXL2BOc+3k5ORzrtFRTU3BOzF8Pjuo9QF8O87BztEd+9wdH/P5wK0wazTGXXXVVYqLi9NHH30UsH337t3q06ePMjMzVVJS4r9kKEk7duxQXFycUlNT1bNnT/Xt21cFBQX+8aamJhUVFSkzM1OSjNQAAAAIBqPBKjo6WlOnTtWzzz6r//iP/9AXX3yh5557Tn/5y180ZcoUjRkzRhdffLEefPBB7dq1S9u2bVN+fr7uuOMO/z1Rd9xxh1544QW9/vrrKi0t1S9+8QvV19fr5ptvliQjNQAAAILB6KVASbrnnnsUExOjp59+WuXl5erXr5+WLl2qq6++WpK0cuVKzZ8/X7fccosuuOAC3Xbbbbrnnnv8+99yyy2qra3VkiVLVFVVpYEDB+qFF17QRRddJOn0TejnWgPnzrI8siyP28toN34qBwDQGYwHK0maMmWKpkyZ0urYpZdeqtWrV3/r/nfeeaf/3dWDVQMdZ1keXXhhLGEFAIBvCEqwQtdmWR6FhVnKW1esQ+W1bi+nXTJSE/WT6we4vQwAQBdHsEKHHSqv1Z7D1W4vo116J/ZwewkAgG6AazkAAACGEKwAAAAMIVgBAAAYQrACAAAwhGAFAABgCMEKAADAEIIVAACAIQQrAAAAQwhWAAAAhhCsAAAADCFYAQAAGEKwAgAAMIRgBQAAYAjBCgAAwBCCFQAAgCEEKwAAAEMIVgAAAIYQrAAAAAwhWAEAABhCsAIAADCEYAUAAGAIwQoAAMAQghUAAIAhBCsAAABDCFYAAACGEKwAAAAMIVgBAAAYQrACAAAwhGAFAABgCMEKAADAEIIVAACAIQQrAAAAQwhWAAAAhhCsAAAADCFYAQAAGEKwAgAAMIRgBQAAYAjBCgAAwBCCFQAAgCEEKwAAAEOCGqz27dun9PR0bdq0yb9t586dysnJUVpamrKzs/Xiiy8G7GPbtp555hmNHDlSaWlpmjZtmg4ePBgwx0QNAAAA04IWrE6dOqXZs2fr5MmT/m3Hjh3TlClT1KdPH23cuFH33nuv8vLytHHjRv+cZcuWaf369Vq4cKE2bNgg27Y1depUNTY2GqsBAAAQDEELVkuXLlWPHj0Ctr366quKiIjQggUL1K9fP02cOFG33367VqxYIUlqbGzU6tWrNWPGDI0ePVqpqal6+umnVVZWpq1btxqrAQAAEAzhwShaWFioV155RZs3b9bo0aP924uKijR8+HCFh//9nx0xYoSef/55HT16VH/729904sQJZWVl+ce9Xq8GDBigwsJCjR8/3kiNjgoPN59Dw8KsgI+hIJTWCrRXKB3XobTWbwrltZ+tUPz63hW43XfjwaqmpkZz5sxRbm6uevXqFTBWVlamlJSUgG2JiYmSpCNHjqisrEySWuyXmJjoHzNRoyMsy6OEhLgO798WrzcmaLUBtI1zsHN0xz53x8d8PnCr78aD1eOPP6709HTdeOONLcbq6+sVGRkZsC0qKkqS1NDQoLq6OklqdU51dbWxGh1h245qak62PfEshYVZ8npjVFNTJ5/PNl4/GJrXDHQlnIOdI5T6fK5C8et7V/D1vsfFRXX6K1dGg9XmzZtVVFSkN998s9Xx6OjoFjeQNzQ0SJJiY2MVHR0t6fR9Us1/b54TExNjrEZHNTUF78Tw+eyg1gfw7TgHO0d37HN3fMznA7fCrNFgtXHjRn311VcB91VJ0mOPPaa33npLycnJqqioCBhr/jwpKUlNTU3+bX369AmY079/f0kyUgMAACAYjAarvLw81dfXB2wbO3asZsyYoR/+8If6/e9/rw0bNsjn8yksLEyStGPHDvXt21c9e/ZUfHy8evTooYKCAn8oqqmpUUlJiXJyciRJmZmZ51wDAAAgGIxeeExKStKll14a8EeSevbsqaSkJE2cOFHHjx/XvHnzVFpaqk2bNmnNmjWaPn26pNP3ReXk5CgvL0/vvPOOdu3apZkzZyo5OVljx46VJCM1AAAAgiEob7dwJj179tTKlSv1xBNPaMKECbr44os1Z84cTZgwwT9nxowZampqUm5ururr65WZmalVq1YpIiLCWA0AAIBgCHqw+uyzzwI+Hzx4sF555ZUzzg8LC9PPf/5z/fznPz/jHBM1AAAATOvUV6xwZqH0BnKhtFYAADoTwcplHo9Htu2E7HvSAACAvyNYucyyPLIsj/LWFetQea3by2mXjNRE/eT6AW4vAwCA8w7B6jxxqLxWew53/J3hO1PvxB5tTwIAoBviZhkAAABDCFYAAACGEKwAAAAMIVgBAAAYQrACAAAwhGAFAABgCMEKAADAEIIVAACAIQQrAAAAQwhWAAAAhvArbQAAOIPm3+faEWFhVsDHzmTbjmzb6fR/FwQrAABaZVkeXXhh7DkHI683xtCK2s/ns1VVdZJw5QKCFQAArbAsj8LCLOWtK9ah8lq3l9NuvZPiNXvSUFmWh2DlAoIVAMidyzUdFUpr7QoOlddqz+Fqt5eBEEGwAtCtXRgfJdt2XLlcA6DrIVgB6NZ6xETIsjwhdbknIzVRP7l+gNvLANAKghUAKLQu9/RO7OH2EgCcARfqAQAADCFYAQAAGEKwAgAAMIRgBQAAYAjBCgAAwBCCFQAAgCEEKwAAAEMIVgAAAIYQrAAAAAwhWAEAABhCsAIAADCEYAUAAGAIwQoAAMAQghUAAIAhBCsAAABDCFYAAACGEKwAAAAMIVgBAAAYQrACAAAwhGAFAABgiPFgVVVVpUcffVSjRo1SRkaGbr31VhUVFfnHt2/frptuuklDhgzRuHHjtGXLloD9GxoaNH/+fGVlZSk9PV0PPfSQKisrA+aYqAEAAGCa8WA1a9Ysffjhh8rPz9fGjRt15ZVX6s4779TevXu1Z88eTZ8+XSNHjtSmTZv04x//WHPmzNH27dv9+z/++ON67733tHTpUq1du1Z79+7VjBkz/OMmagAAAARDuMliBw4c0F/+8hetX79eQ4cOlST98pe/1P/8z//ozTff1FdffaX+/ftr5syZkqR+/fqppKREK1euVFZWlsrLy7V582YtX75cw4YNkyTl5+dr3Lhx+vDDD5Wenq61a9eecw0AAIBgMPqKVUJCglasWKFBgwb5t3k8Hnk8HtXU1KioqEhZWVkB+4wYMULFxcVyHEfFxcX+bc369u2rpKQkFRYWSpKRGgAAAMFg9BUrr9era6+9NmDb22+/rQMHDugXv/iFXn/9dSUnJweMJyYmqq6uTseOHVN5ebkSEhIUFRXVYk5ZWZkkqays7JxrdFR4uPl7/S3LY7wmAJyvwsJC52emQmmtrQn19XdU8+N26/EbDVbf9MEHH+iRRx7R2LFjNXr0aNXX1ysyMjJgTvPnjY2NqqurazEuSVFRUWpoaJAkIzU6wrI8SkiI6/D+AADJ641xewndRnfvtVuPP2jBatu2bZo9e7YyMjKUl5cn6XS4aWxsDJjX/HlMTIyio6NbjEunf8ovJibGWI2OsG1HNTUnO7z/mUREhKlHj2jjdQHgfFRTUyefz3Z7Ge0SFmaFdDgJpV6b1Py81dTUKS4uqtNfuQpKsHr55Zf1xBNPaNy4cfr1r3/tfwWpV69eqqioCJhbUVGh2NhYxcfHKzk5WVVVVWpsbAx41amiokJJSUnGanRUU5P5A7S7vlQLoHvy+eygfC1FS929126FSuPf1devX6+FCxdq0qRJys/PDwg3w4YN0/vvvx8wf8eOHcrIyJBlWRo6dKhs2/bfgC5J+/btU3l5uTIzM43VAAAACAajwWrfvn1atGiRfvCDH2j69Ok6evSovvzyS3355Zeqra3V5MmT9fHHHysvL0979uzR6tWr9Yc//EFTp06VJCUlJemGG25Qbm6uCgoK9PHHH2vWrFkaPny40tLSJMlIDQAAgGAweinw7bff1qlTp/THP/5Rf/zjHwPGJkyYoCeffFLLli3T4sWLtXbtWvXu3VuLFy8OePuEhQsXatGiRbrvvvskSaNGjVJubq5//IorrjjnGgAAAMFgNFjddddduuuuu751zqhRozRq1KgzjsfGxupXv/qVfvWrXwW1BgAAgGncOQ0AAGAIwQoAAMAQghUAAIAhBCsAAABDCFYAAACGEKwAAAAMIVgBAAAYQrACAAAwhGAFAABgCMEKAADAEIIVAACAIUZ/VyAAAN8mLCx0/j8fSmvF+YNgBQAIugvjo2TbjrzeGLeXAgQVwQoAEHQ9YiJkWR7lrSvWofJat5fTLhmpifrJ9QPcXgZCDMEKANBpDpXXas/hareX0S69E3u4vQSEIC4gAwAAGEKwAgAAMIRgBQAAYAjBCgAAwBCCFQAAgCEEKwAAAEMIVgAAAIYQrAAAAAwhWAEAABhCsAIAADCEYAUAAGAIwQoAAMAQghUAAIAhBCsAAABDCFYAAACGEKwAAAAMIVgBAAAYQrACAAAwhGAFAABgCMEKAADAEIIVAACAIQQrAAAAQwhWAAAAhhCsAAAADAl3ewEAAMC8sLDQeu3Eth3ZtuP2Ms4ZwQoAgC7kwvgo2bYjrzfG7aWcFZ/PVlXVyZAPV102WNm2rd/+9rd67bXXVFtbq8zMTD366KP6zne+4/bSAAAImh4xEbIsj/LWFetQea3by2mX3knxmj1pqCzLQ7A6Xy1btkzr16/Xk08+qeTkZC1evFhTp07Vm2++qcjISLeXBwBAUB0qr9Wew9VuL6PbCa0LsO3U2Nio1atXa8aMGRo9erRSU1P19NNPq6ysTFu3bnV7eQAAoIvqksFq165dOnHihLKysvzbvF6vBgwYoMLCQhdXBgAAujKP4zihfTGzFVu3btX999+vjz76SNHR0f7tDzzwgOrr6/X888+fdU3HCc5PK3g8kmVZqqptUJPPNl4/GKIiwxQfG8mag4w1dw7W3DlYc+cJxXWHh1n//6Z7W+eaSpq/r9q2LY/HI4/HY2aR7dQl77Gqq6uTpBb3UkVFRam6umPXmz0ej8LCgvfkXBgfFbTawcKaOwdr7hysuXOw5s4Tiuu2LHMX0kzWOqt/15V/NciaX6VqbGwM2N7Q0KCYmND68VMAABA6umSw6tWrlySpoqIiYHtFRYWSkpLcWBIAAOgGumSwSk1NVY8ePVRQUODfVlNTo5KSEmVmZrq4MgAA0JV1yXusIiMjlZOTo7y8PF100UW65JJLtHjxYiUnJ2vs2LFuLw8AAHRRXTJYSdKMGTPU1NSk3Nxc1dfXKzMzU6tWrVJERITbSwMAAF1Ul3y7BQAAADd0yXusAAAA3ECwAgAAMIRgBQAAYAjBCgAAwBCCFQAAgCEEKwAAAEMIVgAAAIYQrFxi27aeeeYZjRw5UmlpaZo2bZoOHjzo9rJCSnl5ufr379/iz6ZNmyRJO3fuVE5OjtLS0pSdna0XX3wxYP/2PAdt1ehunn/+eU2ePDlgW2f0ubufL631PTc3t8Wxn52d7R+n7x1TVVWlRx99VKNGjVJGRoZuvfVWFRUV+ce3b9+um266SUOGDNG4ceO0ZcuWgP0bGho0f/58ZWVlKT09XQ899JAqKysD5pio0dW01fcpU6a0ON6/fk6cV3134IqlS5c6V199tfOnP/3J2blzp3PHHXc4Y8eOdRoaGtxeWsh49913nUGDBjnl5eVORUWF/09dXZ1TWVnpXH311c4jjzzilJaWOr/73e+cQYMGOb/73e/8+7f1HLSnRnfy8ssvO6mpqU5OTo5/W2f1uTufL6313XEc5+abb3by8/MDjv2vvvrKP07fO2bKlCnO+PHjncLCQmfv3r3O/PnzncGDBzt79uxxSktLnUGDBjn5+flOaWmps3LlSmfAgAHO//7v//r3f/jhh50xY8Y4hYWFzkcffeT86Ec/ciZNmuQfN1GjK/q2vjuO42RlZTnr168PON6PHTvm3/986jvBygUNDQ1Oenq6s27dOv+26upqZ/Dgwc6bb77p4spCy4oVK5wbb7yx1bHly5c711xzjXPq1Cn/tn/7t39zxo4d6zhO+56Dtmp0F2VlZc706dOdtLQ0Z9y4cQHf4Dujz931fPm2vtu27aSlpTlbt25tdV/63jH79+93UlJSnKKiIv8227adMWPGOEuWLHF++ctfOjfffHPAPrNmzXLuuOMOx3FOP2epqanOu+++6x/fu3evk5KS4nzwwQeO4zhGanQ1bfX96NGjTkpKivPpp5+2uv/51ncuBbpg165dOnHihLKysvzbvF6vBgwYoMLCQhdXFlo+++wz9evXr9WxoqIiDR8+XOHhf/91mCNGjND+/ft19OjRdj0HbdXoLj799FNFRETojTfe0JAhQwLGOqPP3fV8+ba+f/HFFzp58qT+6Z/+qdV96XvHJCQkaMWKFRo0aJB/m8fjkcfjUU1NjYqKigL6IZ3uWXFxsRzHUXFxsX9bs759+yopKSmg7+dao6tpq++fffaZPB6P+vbt2+r+51vfCVYuKCsrkyT16tUrYHtiYqJ/DG3bvXu3KisrNWnSJH33u9/Vrbfeqj//+c+STvc4OTk5YH5iYqIk6ciRI+16Dtqq0V1kZ2dr6dKl+s53vtNirDP63F3Pl2/r++7duyVJL730krKzszVmzBgtWLBAtbW1ktr3NYa+t+T1enXttdcqMjLSv+3tt9/WgQMHNHLkyDP2rK6uTseOHVN5ebkSEhIUFRXVYk5bfT+bGl1NW33fvXu34uPjtWDBAo0aNUrjxo3TkiVL1NjYKEnnXd8JVi6oq6uTpICDSJKioqLU0NDgxpJCTlNTk/bu3avq6mrdf//9WrFihdLS0vSzn/1M27dvV319fav9lU7foNie56CtGmi7Ryb6zPnS0u7du2VZlhITE7V8+XI9/PDDeu+993TPPffItm36bsgHH3ygRx55RGPHjtXo0aNb7Vnz542Njaqrq2sxLrXd97Ot0dV9s++7d+9WQ0ODBg8erJUrV+ruu+/Wa6+9ptzcXEk67/oe3vYUmBYdHS3p9JPZ/Hfp9BezmJgYt5YVUsLDw1VQUKCwsDB/DwcOHKjPP/9cq1atUnR0tP9/M82aT47Y2Nh2PQdt1UDbPTLRZ86Xlu6++27ddtttSkhIkCSlpKTo4osv1i233KJPPvmEvhuwbds2zZ49WxkZGcrLy5N0+pvsN3vW/HlMTEyrPZUCe2aiRlfWWt8XLFiguXPn6oILLpB0+niPiIjQzJkzNWfOnPOu77xi5YLml9YrKioCtldUVCgpKcmNJYWkuLi4gC/4knTFFVeovLxcycnJrfZXkpKSktr1HLRVA233yESfOV9asizLH6qaXXHFFZJOX/Kg7+fm5Zdf1v3336/rrrtOy5cv97+S16tXr1b7ERsbq/j4eCUnJ6uqqqrFN+iv98xEja7qTH0PDw/3h6pmXz/ez7e+E6xckJqaqh49eqigoMC/raamRiUlJcrMzHRxZaHj888/V0ZGRkAPJen//u//dPnllyszM1PFxcXy+Xz+sR07dqhv377q2bNnu56Dtmqg7R6Z6DPnS0tz5szR7bffHrDtk08+kSRdfvnl9P0crF+/XgsXLtSkSZOUn58fcHlo2LBhev/99wPm79ixQxkZGbIsS0OHDpVt2/4boSVp3759Ki8v9/fMRI2u6Nv6PnnyZD3yyCMB8z/55BNFRETosssuO//6flY/Qwhj8vPzneHDhzvbtm0LeH+YxsZGt5cWEnw+nzNx4kTn+uuvdwoLC53S0lJn0aJFzsCBA53PPvvMOXr0qJOZmenMnTvX+fzzz52NGzc6gwYNcjZt2uSv0dZz0J4a3c3cuXMDfuy/s/rc3c+Xb/Z927ZtTkpKirN06VLnwIEDzrvvvutkZ2c7s2bN8s+h72dv7969zlVXXeXce++9Ae+XVFFR4dTU1Di7d+92rrrqKmfx4sVOaWmps2rVqhbvhTRr1iwnOzvb2bFjh/+9kL7+3Jmo0dW01feXXnrJufLKK53169c7X3zxhbNlyxbn6quvdvLz8/01zqe+E6xc0tTU5Dz11FPOiBEjnLS0NGfatGnOwYMH3V5WSPnyyy+dhx9+2Pne977nDBo0yPmXf/kXp7Cw0D/+0UcfObfccoszcOBA57rrrnNeeumlgP3b8xy0VaO7+eY3eMfpnD539/Oltb6/9dZbzo9+9CNn8ODBzve+9z3nySefdOrr6/3j9P3sPffcc05KSkqrf+bOnes4juP893//tzN+/Hhn4MCBzrhx45wtW7YE1Dhx4oQzb948Z9iwYc6wYcOcWbNmOZWVlQFzTNToStrT95dfftn553/+Z/+x+txzzzk+n89f43zqu8dxHOfsXuMCAABAa7jHCgAAwBCCFQAAgCEEKwAAAEMIVgAAAIYQrAAAAAwhWAEAABhCsAIAADCEYAUAAGAIwQoAAMAQghUAAIAhBCsAAABD/h98DDxSnUbxdgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.Purchase.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.boxplot(column=['New_Age'])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User_ID                       0.000000\n",
       "Product_ID                    0.000000\n",
       "Gender                        0.000000\n",
       "Age                           0.000000\n",
       "Occupation                    0.000000\n",
       "City_Category                 0.000000\n",
       "Stay_In_Current_City_Years    0.000000\n",
       "Marital_Status                0.000000\n",
       "Product_Category_1            0.000000\n",
       "Product_Category_2            0.315666\n",
       "Product_Category_3            0.696727\n",
       "Purchase                      0.000000\n",
       "New_Age                       0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()/df.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data manipulation for each type of model \n",
    "## Linear Regression\n",
    "    # No missing values \n",
    "    # Remove outliers \n",
    "# Ridge\n",
    "# Lasso \n",
    "# Elastic Net \n",
    "## KNN \n",
    "    # feature scaling \n",
    "    # Imputation \n",
    "## SVM\n",
    "    # Remove Outliers \n",
    "    # Remove missing values \n",
    "    # Scaling \n",
    "## Decision Tree\n",
    "## Random Forest\n",
    "## Gradient Boosted Tree\n",
    "## XG Boost\n",
    "## ANN  \n",
    "    # Imputation \n",
    "    # scaling\n",
    "    \n",
    "# fix scaling / avoid leakage --> pipelines? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\" **Data Preprocessing**\"\n",
    "\n",
    "(1) Data imputation\n",
    "\n",
    "(2) Imputing & Outlier Removal\n",
    "\n",
    "(3) Imputing and Scaling\n",
    "\n",
    "(4) Imputing, Scaling, and Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['User_ID', 'Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category',\n",
       "       'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1',\n",
       "       'Product_Category_2', 'Product_Category_3', 'Purchase', 'New_Age'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill imputed categories with value indicating it is nan - 21\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "cols_to_impute = ['Product_Category_1',\n",
    "                  'Product_Category_2',\n",
    "                  'Product_Category_3']\n",
    "\n",
    "cols_to_category = ['Gender',\n",
    "                    'Occupation',\n",
    "                    'City_Category',\n",
    "                    'Marital_Status',\n",
    "                    'Product_Category_1',\n",
    "                    'Product_Category_2',\n",
    "                    'Product_Category_3']\n",
    "\n",
    "cols_to_scale = ['New_Age',\n",
    "                 'Stay_In_Current_City_Years']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute NaN's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categorical_imputer = SimpleImputer(strategy='constant', fill_value=21)\n",
    "\n",
    "impute_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat_impute', categorical_imputer, cols_to_impute),\n",
    "    ],remainder = 'passthrough',\n",
    "    verbose_feature_names_out= False).set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert Numeric to Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:343: UserWarning: With transform=\"pandas\", `func` should return a DataFrame to follow the set_output API.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def to_category(x):\n",
    "    return pd.DataFrame(x).astype(\"category\")\n",
    "\n",
    "to_category = FunctionTransformer(to_category)\n",
    "\n",
    "category_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat_trans', to_category, cols_to_category),\n",
    "    ],remainder = 'passthrough', verbose_feature_names_out= False).set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "#adjust for X & Y \n",
    "\n",
    "def z_score_removal(X,y, columns, z_score):\n",
    "    df = pd.concat([X, y], axis=1)\n",
    "    col_df = df[columns]    \n",
    "    z_scores = scipy.stats.zscore(col_df).abs()\n",
    "    outliers = (z_scores.max(axis=1) > z_score)\n",
    "    df_out = df[~outliers]\n",
    "    X_cleaned = df_out[X.columns]\n",
    "    y_cleaned = df_out.drop(X.columns, axis =1)\n",
    "    return X_cleaned, y_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "## Feature Scaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "scaling_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('min_max', scaler, cols_to_scale),\n",
    "    ],remainder = 'passthrough', verbose_feature_names_out= False).set_output(transform='pandas')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dummy variables \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oh_encoder = OneHotEncoder(sparse_output=False, drop='first',handle_unknown='ignore')\n",
    "onehot_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', oh_encoder, cols_to_category),\n",
    "    ],remainder = 'passthrough', verbose_feature_names_out= False).set_output(transform='pandas')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train/ test split \n",
    "X = df.drop(['User_ID','Product_ID','Age','Purchase'], axis=1)\n",
    "y = df.loc[:,'Purchase']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########Data Impuation & Outlier Removal (Linear Regression) ###############\n",
    "#outlier removal function\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "reg_pipe = Pipeline([\n",
    "    ('impute_trans', impute_preprocessor),\n",
    "    ('categorical_trans', category_preprocessor),\n",
    "    ('onehot', onehot_preprocessor)\n",
    "])\n",
    "\n",
    "X_Reg = reg_pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modeling**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only remove outliers from the training set\n",
    "X_Reg_fin, y_reg_fin = z_score_removal(X_Reg,y_train,['New_Age','Purchase'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running linear regression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "sm_X = sm.add_constant(X_Reg_fin)\n",
    "sm_model_lr = sm.OLS(y_reg_fin, sm_X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               Purchase   R-squared:                       0.619\n",
      "Model:                            OLS   Adj. R-squared:                  0.619\n",
      "Method:                 Least Squares   F-statistic:                     8404.\n",
      "Date:                Thu, 26 Dec 2024   Prob (F-statistic):               0.00\n",
      "Time:                        14:37:57   Log-Likelihood:            -3.7140e+06\n",
      "No. Observations:              397858   AIC:                         7.428e+06\n",
      "Df Residuals:                  397780   BIC:                         7.429e+06\n",
      "Df Model:                          77                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================================\n",
      "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "const                       1.245e+04    136.649     91.122      0.000    1.22e+04    1.27e+04\n",
      "Gender_M                     -48.9900     10.581     -4.630      0.000     -69.728     -28.252\n",
      "Occupation_1                 -22.5045     19.281     -1.167      0.243     -60.294      15.285\n",
      "Occupation_2                   7.6531     23.028      0.332      0.740     -37.482      52.788\n",
      "Occupation_3                 184.0979     27.121      6.788      0.000     130.941     237.255\n",
      "Occupation_4                 126.0642     17.443      7.227      0.000      91.877     160.251\n",
      "Occupation_5                  48.1086     30.929      1.555      0.120     -12.510     108.728\n",
      "Occupation_6                 195.3061     25.640      7.617      0.000     145.053     245.559\n",
      "Occupation_7                  91.1535     18.038      5.053      0.000      55.799     126.508\n",
      "Occupation_8                -168.2889     85.458     -1.969      0.049    -335.785      -0.793\n",
      "Occupation_9                 148.1697     42.440      3.491      0.000      64.989     231.351\n",
      "Occupation_10                155.9046     40.593      3.841      0.000      76.344     235.465\n",
      "Occupation_11                 83.3109     31.913      2.611      0.009      20.762     145.860\n",
      "Occupation_12                261.6811     21.777     12.017      0.000     219.000     304.362\n",
      "Occupation_13                -35.2815     54.437     -0.648      0.517    -141.977      71.414\n",
      "Occupation_14                207.0081     22.977      9.009      0.000     161.973     252.043\n",
      "Occupation_15                326.8803     31.694     10.314      0.000     264.762     388.999\n",
      "Occupation_16                169.3454     23.852      7.100      0.000     122.596     216.095\n",
      "Occupation_17                199.1972     20.272      9.826      0.000     159.464     238.930\n",
      "Occupation_18                 65.8762     40.613      1.622      0.105     -13.723     145.476\n",
      "Occupation_19               -317.8750     37.752     -8.420      0.000    -391.867    -243.883\n",
      "Occupation_20               -128.4156     21.199     -6.058      0.000    -169.965     -86.867\n",
      "City_Category_B              110.8230     10.699     10.359      0.000      89.854     131.792\n",
      "City_Category_C              457.5136     11.728     39.009      0.000     434.526     480.501\n",
      "Marital_Status_1             -53.3582      9.281     -5.749      0.000     -71.549     -35.167\n",
      "Product_Category_1_2.0     -1166.2013     29.111    -40.061      0.000   -1223.257   -1109.146\n",
      "Product_Category_1_3.0     -1637.4889     40.312    -40.620      0.000   -1716.499   -1558.478\n",
      "Product_Category_1_4.0     -1.029e+04     36.461   -282.277      0.000   -1.04e+04   -1.02e+04\n",
      "Product_Category_1_5.0     -6606.7206     16.383   -403.271      0.000   -6638.830   -6574.611\n",
      "Product_Category_1_6.0       816.2463     31.845     25.632      0.000     753.831     878.662\n",
      "Product_Category_1_7.0      1418.3961     65.385     21.693      0.000    1290.244    1546.548\n",
      "Product_Category_1_8.0     -5358.7280     17.389   -308.170      0.000   -5392.810   -5324.646\n",
      "Product_Category_1_9.0      1009.7140    174.898      5.773      0.000     666.919    1352.509\n",
      "Product_Category_1_10.0     3838.5523     63.016     60.914      0.000    3715.043    3962.061\n",
      "Product_Category_1_11.0    -8089.5331     24.264   -333.392      0.000   -8137.091   -8041.976\n",
      "Product_Category_1_12.0    -1.148e+04     53.356   -215.161      0.000   -1.16e+04   -1.14e+04\n",
      "Product_Category_1_13.0    -1.218e+04     44.836   -271.640      0.000   -1.23e+04   -1.21e+04\n",
      "Product_Category_1_14.0      276.8880     82.383      3.361      0.001     115.419     438.357\n",
      "Product_Category_1_15.0     -322.6791     48.389     -6.668      0.000    -417.520    -227.838\n",
      "Product_Category_1_16.0      550.3312     39.066     14.087      0.000     473.764     626.898\n",
      "Product_Category_1_17.0    -2723.8787    134.493    -20.253      0.000   -2987.481   -2460.276\n",
      "Product_Category_1_18.0    -9853.0816     59.077   -166.785      0.000   -9968.870   -9737.293\n",
      "Product_Category_1_19.0    -1.287e+04     81.310   -158.332      0.000    -1.3e+04   -1.27e+04\n",
      "Product_Category_1_20.0    -1.252e+04     64.884   -192.931      0.000   -1.26e+04   -1.24e+04\n",
      "Product_Category_2_3.0      1230.5013     93.708     13.131      0.000    1046.836    1414.166\n",
      "Product_Category_2_4.0     -1514.0642     41.412    -36.561      0.000   -1595.230   -1432.899\n",
      "Product_Category_2_5.0      -580.2259     31.338    -18.515      0.000    -641.647    -518.805\n",
      "Product_Category_2_6.0        -7.3227     32.106     -0.228      0.820     -70.249      55.603\n",
      "Product_Category_2_7.0       717.3594    128.010      5.604      0.000     466.464     968.254\n",
      "Product_Category_2_8.0       252.0134     26.046      9.676      0.000     200.965     303.062\n",
      "Product_Category_2_9.0      -187.8105     48.040     -3.909      0.000    -281.967     -93.654\n",
      "Product_Category_2_10.0      963.0133     80.645     11.941      0.000     804.952    1121.074\n",
      "Product_Category_2_11.0     -368.3696     33.769    -10.908      0.000    -434.557    -302.183\n",
      "Product_Category_2_12.0     -397.6438     49.243     -8.075      0.000    -494.159    -301.129\n",
      "Product_Category_2_13.0     -179.1160     40.320     -4.442      0.000    -258.143    -100.089\n",
      "Product_Category_2_14.0     -136.5850     27.709     -4.929      0.000    -190.893     -82.277\n",
      "Product_Category_2_15.0     -193.2416     27.589     -7.004      0.000    -247.315    -139.168\n",
      "Product_Category_2_16.0      143.6561     28.001      5.130      0.000      88.775     198.537\n",
      "Product_Category_2_17.0      440.2108     36.941     11.917      0.000     367.807     512.614\n",
      "Product_Category_2_18.0      462.5695     66.486      6.957      0.000     332.259     592.880\n",
      "Product_Category_2_21.0      -31.4756     25.599     -1.230      0.219     -81.649      18.698\n",
      "Product_Category_3_4.0     -3447.5051    166.511    -20.704      0.000   -3773.862   -3121.149\n",
      "Product_Category_3_5.0       340.4706    137.878      2.469      0.014      70.233     610.708\n",
      "Product_Category_3_6.0        -4.1370    143.875     -0.029      0.977    -286.128     277.854\n",
      "Product_Category_3_8.0      1231.1262    139.184      8.845      0.000     958.330    1503.922\n",
      "Product_Category_3_9.0        97.5434    138.930      0.702      0.483    -174.754     369.841\n",
      "Product_Category_3_10.0     -668.8255    167.798     -3.986      0.000    -997.704    -339.947\n",
      "Product_Category_3_11.0     -548.4076    154.625     -3.547      0.000    -851.469    -245.346\n",
      "Product_Category_3_12.0       85.4580    140.518      0.608      0.543    -189.953     360.869\n",
      "Product_Category_3_13.0     -592.5718    144.183     -4.110      0.000    -875.167    -309.977\n",
      "Product_Category_3_14.0      -91.2953    137.988     -0.662      0.508    -361.747     179.157\n",
      "Product_Category_3_15.0     -405.0340    136.425     -2.969      0.003    -672.423    -137.645\n",
      "Product_Category_3_16.0     -104.7131    137.335     -0.762      0.446    -373.886     164.460\n",
      "Product_Category_3_17.0      620.2919    138.898      4.466      0.000     348.057     892.527\n",
      "Product_Category_3_18.0      -32.7710    144.319     -0.227      0.820    -315.631     250.089\n",
      "Product_Category_3_21.0     -219.0058    136.419     -1.605      0.108    -486.383      48.372\n",
      "Stay_In_Current_City_Years     7.3801      3.384      2.181      0.029       0.748      14.012\n",
      "New_Age                       10.7612      0.513     20.965      0.000       9.755      11.767\n",
      "==============================================================================\n",
      "Omnibus:                    32902.387   Durbin-Watson:                   1.998\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            49849.063\n",
      "Skew:                          -0.653   Prob(JB):                         0.00\n",
      "Kurtosis:                       4.142   Cond. No.                     4.39e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.39e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "print(sm_model_lr.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model_lr = LinearRegression()\n",
    "\n",
    "model_lr.fit(X_Reg_fin, y_reg_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7518479.54491493 -7523014.62981981 -7501877.76204618]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "lr_scores = cross_val_score(model_lr, X_Reg_fin, y_reg_fin, cv=3, scoring='neg_mean_squared_error')\n",
    "print(lr_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########Data Impuation, Outlier Removal, & Scaling (Ridge, Lasso, Elastic NEt, SVM) ###############\n",
    "rr_pipe = Pipeline([\n",
    "    ('impute_trans', impute_preprocessor),\n",
    "    ('categorical_trans', category_preprocessor),\n",
    "    ('feat_scaling', scaling_preprocessor),\n",
    "    ('onehot', onehot_preprocessor)\n",
    "])\n",
    "\n",
    "X_rr = rr_pipe.fit_transform(X_train)\n",
    "X_rr_fin, y_rr_fin = z_score_removal(X_rr,y_train,['New_Age','Purchase'],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression + Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7523045.22039712 -7531709.14666194 -7515464.14871019]\n",
      "[-7518359.80156523 -7523057.36949733 -7501911.23763842]\n",
      "[-16448558.32772285 -16648036.7447575  -16602902.50038427]\n"
     ]
    }
   ],
   "source": [
    "#Code for running ridge regression \n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet \n",
    "\n",
    "lasso_model = Lasso()\n",
    "ridge_model = Ridge()\n",
    "en_model = ElasticNet() \n",
    "\n",
    "la_scores = cross_val_score(lasso_model, X_rr_fin, y_rr_fin, cv=3, scoring='neg_mean_squared_error')\n",
    "print(la_scores)\n",
    "r_scores = cross_val_score(ridge_model, X_rr_fin, y_rr_fin, cv=3, scoring='neg_mean_squared_error')\n",
    "print(r_scores)\n",
    "en_scores = cross_val_score(en_model, X_rr_fin, y_rr_fin, cv=3, scoring='neg_mean_squared_error')\n",
    "print(en_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Lasso and Ridge Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 vs L2 Regularization: L1 regularization (Lasso) tends to produce sparse solutions, driving some coefficients to zero and performing feature selection, while L2 regularization (Ridge) produces solutions with small coefficients but does not drive them to zero, distributing the weights among correlated features.\n",
    "\n",
    "**Relevant Parameters**:\n",
    "\n",
    "**alpha**: Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization.\n",
    "\n",
    "**fit_intercept**: Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e., data is expected to be centered).\n",
    "\n",
    "**normalize**: This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the L2-norm.\n",
    "\n",
    "**max_iter**: Maximum number of iterations for conjugate gradient solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha value: 1\n",
      "Best negative mean squared error: -7514625.885351087\n",
      "Best model test R^2 score: 0.6193012318033952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.134e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.174e+12, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.138e+12, tolerance: 6.278e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.146e+12, tolerance: 6.274e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.089e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.530e+11, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.999e+11, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.635e+11, tolerance: 6.278e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.200e+11, tolerance: 6.274e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.351e+09, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.905e+10, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.020e+11, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.497e+10, tolerance: 6.278e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.167e+11, tolerance: 6.274e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.607e+08, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.804e+08, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha value: 0.01\n",
      "Best negative mean squared error: -7514582.044296277\n",
      "Best model test R^2 score: 0.6193011998792641\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "alpha_values = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "param_grid = {'alpha': alpha_values}\n",
    "grid_search_ridge = GridSearchCV(ridge_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_ridge.fit(X_rr_fin, y_rr_fin)\n",
    "test_score_ridge = grid_search_ridge.best_estimator_.score(X_rr_fin, y_rr_fin)\n",
    "\n",
    "\n",
    "print(f\"Best alpha value: {grid_search_ridge.best_params_['alpha']}\")\n",
    "print(f\"Best negative mean squared error: {grid_search_ridge.best_score_}\")\n",
    "print(f\"Best model test R^2 score: {test_score_ridge}\")\n",
    "\n",
    "\n",
    "param_grid = {'alpha': alpha_values}\n",
    "grid_search_lasso = GridSearchCV(lasso_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_lasso.fit(X_rr_fin, y_rr_fin)\n",
    "test_score_lasso = grid_search_lasso.best_estimator_.score(X_rr_fin, y_rr_fin)\n",
    "\n",
    "print(f\"Best alpha value: {grid_search_lasso.best_params_['alpha']}\")\n",
    "print(f\"Best negative mean squared error: {grid_search_lasso.best_score_}\")\n",
    "print(f\"Best model test R^2 score: {test_score_lasso}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Elastic Net Regression\n",
    "Elastic Net Regression is a linear regression model that combines L1 and L2 regularization, balancing the sparsity-inducing property of Lasso and the smoothness of Ridge Regression. It is particularly useful when there are multiple correlated features in the dataset.\n",
    "\n",
    "Relevant Parameters:\n",
    "\n",
    "**alpha**: Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. alpha is the combined weight of both L1 and L2 regularization terms.\n",
    "\n",
    "**l1_ratio**: The mixing parameter between L1 and L2 regularization, with a value between 0 and 1. A value of 0 corresponds to Ridge Regression (pure L2 penalty), and a value of 1 corresponds to Lasso Regression (pure L1 penalty). Values between 0 and 1 give a mix of both L1 and L2 regularization.\n",
    "\n",
    "**fit_intercept**: Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e., data is expected to be centered).\n",
    "normalize: This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the L2-norm.\n",
    "\n",
    "**max_iter**: Maximum number of iterations for the coordinate descent solver.\n",
    "\n",
    "By tuning alpha and l1_ratio, you can control the balance between L1 and L2 regularization and find the optimal trade-off for your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.195e+12, tolerance: 6.287e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.198e+12, tolerance: 6.284e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.197e+12, tolerance: 6.278e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.195e+12, tolerance: 6.274e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.198e+12, tolerance: 6.275e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.184e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.194e+12, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.187e+12, tolerance: 6.278e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.186e+12, tolerance: 6.274e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.056e+12, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.174e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.190e+12, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.177e+12, tolerance: 6.278e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.178e+12, tolerance: 6.274e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.338e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.165e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.187e+12, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.169e+12, tolerance: 6.278e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.171e+12, tolerance: 6.274e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.158e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.184e+12, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.162e+12, tolerance: 6.278e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.166e+12, tolerance: 6.274e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.265e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.152e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.182e+12, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.156e+12, tolerance: 6.278e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.161e+12, tolerance: 6.274e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.348e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.147e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.180e+12, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.151e+12, tolerance: 6.278e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.157e+12, tolerance: 6.274e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.476e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.143e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.178e+12, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.147e+12, tolerance: 6.278e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.153e+12, tolerance: 6.274e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.629e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.139e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.177e+12, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.144e+12, tolerance: 6.278e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.150e+12, tolerance: 6.274e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.788e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.136e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.175e+12, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.141e+12, tolerance: 6.278e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.148e+12, tolerance: 6.274e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.943e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.134e+12, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.174e+12, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.138e+12, tolerance: 6.278e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.146e+12, tolerance: 6.274e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.089e+11, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.209e+12, tolerance: 6.287e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.212e+12, tolerance: 6.284e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.211e+12, tolerance: 6.278e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.209e+12, tolerance: 6.274e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.212e+12, tolerance: 6.275e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.046e+09, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.909e+10, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.596e+10, tolerance: 6.278e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.157e+10, tolerance: 6.274e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.101e+09, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.994e+09, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.972e+10, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.393e+10, tolerance: 6.278e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.120e+10, tolerance: 6.274e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.030e+09, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.010e+10, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.087e+11, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.041e+10, tolerance: 6.278e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.796e+10, tolerance: 6.274e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.751e+09, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.065e+10, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.691e+11, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.633e+10, tolerance: 6.278e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.198e+10, tolerance: 6.274e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.576e+09, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.517e+10, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.693e+11, tolerance: 6.284e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.957e+10, tolerance: 6.278e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.320e+10, tolerance: 6.274e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.849e+09, tolerance: 6.275e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\sasi virat\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.003e+11, tolerance: 6.287e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "## Elastic net \n",
    "alpha_values = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 10000]\n",
    "l1_ratios = np.linspace(0, 1, 11) \n",
    "\n",
    "param_grid_en = {'alpha': alpha_values, 'l1_ratio': l1_ratios}\n",
    "grid_search_en = GridSearchCV(en_model, param_grid_en, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_en.fit(X_rr_fin, y_rr_fin)\n",
    "test_score_en = grid_search_en.best_estimator_.score(X_rr_fin, y_rr_fin)\n",
    "\n",
    "\n",
    "print(f\"Best alpha value: {grid_search_en.best_params_['alpha']}\")\n",
    "print(f\"Best l1 ratio value: {grid_search_en.best_params_['l1_ratio']}\")\n",
    "print(f\"Best negative mean squared error: {grid_search_en.best_score_}\")\n",
    "print(f\"Best model test R^2 score: {test_score_en}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_search_en' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest l1 ratio value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search_en\u001b[38;5;241m.\u001b[39mbest_params_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grid_search_en' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Best l1 ratio value: {grid_search_en.best_params_['l1_ratio']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Hyperparameter Tuning - SVM Regression\n",
    "Support Vector Machine (SVM) Regression is a versatile machine learning algorithm that can be used for both linear and non-linear regression tasks. It aims to find the best-fitting hyperplane that has the largest distance (margin) between the support vectors and the hyperplane.\n",
    "\n",
    "Relevant Parameters:\n",
    "\n",
    "**kernel**: Specifies the kernel function to be used in the algorithm. Possible options are 'linear', 'poly', 'rbf', 'sigmoid', and 'precomputed'. The choice of the kernel function depends on the nature of the data and the problem to be solved.\n",
    " \n",
    " ***C**: Regularization parameter (also called the cost parameter); must be a positive float. It determines the trade-off between achieving a low training error and a low testing error. In other words, it controls the balance between overfitting and underfitting. A smaller value of C creates a wider margin, which may result in more training errors but better generalization to the test data. A larger value of C creates a narrower margin, which may result in fewer training errors but poorer generalization to the test data.\n",
    "\n",
    "**degree**: The degree of the polynomial kernel function ('poly'). Ignored by all other kernels. It is the degree of the polynomial used for the 'poly' kernel and determines the flexibility of the model.\n",
    "\n",
    "**gamma**: Kernel coefficient for 'rbf', 'poly', and 'sigmoid'. If gamma is 'scale' (default), then it is calculated as 1 / (n_features * X.var()) for the input data X. If gamma is 'auto', then it is calculated as 1/n_features. A smaller gamma value will produce a more flexible model, while a larger gamma value will produce a more rigid model.\n",
    "\n",
    "**coef0**: Independent term in the kernel function. It is only significant in 'poly' and 'sigmoid'. It controls the influence of higher degree terms in the polynomial and sigmoid kernels.\n",
    "\n",
    "**shrinking**: Whether to use the shrinking heuristic. The shrinking heuristic is a technique used to speed up training by removing some of the support vectors that are not necessary for the final solution. True by default.\n",
    "\n",
    "**epsilon**: Epsilon parameter in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated with the training loss function on the prediction error. Larger values of epsilon result in more tolerance for errors.\n",
    "\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific regression problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM Code \n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_model = SVR(kernel='linear', C=1.0)\n",
    "svm_scores = cross_val_score(svm_model, X_rr_fin, y_rr_fin, cv=5,scoring='neg_mean_squared_error')\n",
    "print(svm_scores)\n",
    "\n",
    "\"\"\"param_grid = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10]\n",
    "}\n",
    "grid_search_svm = GridSearchCV(svr, param_grid, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_svm.fit(X_rr_fin, y_rr_fin)\n",
    "test_score_svm = grid_search_svm.best_estimator_.score(X_rr_fin, y_rr_fin)\n",
    "\n",
    "\n",
    "print(f\"Best kernel value: {grid_search_svm.best_params_['kernel']}\")\n",
    "print(f\"Best C value: {grid_search_svm.best_params_['C']}\")\n",
    "print(f\"Best gamma value: {grid_search_svm.best_params_['gamma']}\")\n",
    "print(f\"Best negative mean squared error: {grid_search_svm.best_score_}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### No Data Manipulation (Decision Tree, Random Forest, XGBoost) ###############\n",
    "tree_pipe = Pipeline([\n",
    "    ('categorical_trans', category_preprocessor),\n",
    "    ('onehot', onehot_preprocessor)\n",
    "])\n",
    "\n",
    "X_tree = tree_pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning - Decision Tree\n",
    "Decision Trees are a popular machine learning algorithm used for both regression and classification tasks. They are easy to interpret and can naturally handle a mixture of continuous and categorical variables.\n",
    "\n",
    "Relevant Parameters:\n",
    "\n",
    "\n",
    "**criterion**: The function to measure the quality of a split. Supported criteria for regression are 'mse' (mean squared error) and 'friedman_mse' (improvement in mean squared error). For classification, supported criteria are 'gini' and 'entropy'.\n",
    "\n",
    "**splitter**: The strategy used to choose the split at each node. Supported strategies are 'best' to choose the best split and 'random' to choose the best random split.\n",
    "\n",
    "**max_depth**: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Controlling the depth can help prevent overfitting.\n",
    "\n",
    "**min_samples_split**: The minimum number of samples required to split an internal node. A larger value prevents the tree from growing too deep, thus preventing overfitting.\n",
    "\n",
    "**min_samples_leaf**: The minimum number of samples required to be at a leaf node. A larger value prevents the tree from growing too deep, thus preventing overfitting.\n",
    "\n",
    "**min_weight_fraction_leaf**: The minimum weighted fraction of the sum total of weights required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "\n",
    "**max_features**: The number of features to consider when looking for the best split. If None, then max_features=n_features.\n",
    "\n",
    "**max_leaf_nodes**: Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None, then unlimited number of leaf nodes.\n",
    "\n",
    "**min_impurity_decrease**: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "\n",
    "**min_impurity_split**: Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision Tree model \n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Decision Tree\n",
    "dt_model = DecisionTreeRegressor()\n",
    "dt_scores = cross_val_score(dt_model, X_tree, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(dt_scores)\n",
    "\n",
    "\"\"\"\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10, 15],\n",
    "    'min_weight_fraction_leaf': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "    'max_leaf_nodes': [None, 10, 20, 30, 40, 50],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "grid_search_dt = GridSearchCV(dt_model, param_grid_dt, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_dt.fit(X_tree, y_train)\n",
    "test_score_dt = grid_search_dt.best_estimator_.score(X_tree, y_train)\n",
    "\n",
    "print(f\"Best max_depth value: {grid_search_dt.best_params_['max_depth']}\")\n",
    "print(f\"Best min_samples_split value: {grid_search_dt.best_params_['min_samples_split']}\")\n",
    "print(f\"Best min_samples_leaf value: {grid_search_dt.best_params_['min_samples_leaf']}\")\n",
    "print(f\"Best negative mean squared error: {grid_search_dt.best_score_}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Random Forest\n",
    "Random Forest is an ensemble learning method that constructs a multitude of decision trees at training time and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. It is highly flexible and can handle a wide variety of tasks.\n",
    "\n",
    "Relevant Parameters:\n",
    "\n",
    "**n_estimators**: The number of trees in the forest. Increasing the number of trees can improve the model's performance, but may also increase the computation time.\n",
    "\n",
    "**criterion**: The function to measure the quality of a split. Supported criteria for regression are 'mse' (mean squared error) and 'mae' (mean absolute error). For classification, supported criteria are 'gini' and 'entropy'.\n",
    "\n",
    "**max_depth**: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Controlling the depth can help prevent overfitting.\n",
    "\n",
    "**min_samples_split**: The minimum number of samples required to split an internal node. A larger value prevents the tree from growing too deep, thus preventing overfitting.\n",
    "\n",
    "**min_samples_leaf**: The minimum number of samples required to be at a leaf node. A larger value prevents the tree from growing too deep, thus preventing overfitting.\n",
    "\n",
    "**min_weight_fraction_leaf**: The minimum weighted fraction of the sum total of weights required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "\n",
    "**max_features**: The number of features to consider when looking for the best split. If None, then max_features=n_features. It can also be a float, int, or string ('auto', 'sqrt', or 'log2').\n",
    "\n",
    "**max_leaf_nodes**: Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None, then unlimited number of leaf nodes.\n",
    "\n",
    "**min_impurity_decrease**: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "\n",
    "**bootstrap**: Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\n",
    "\n",
    "**oob_score**: Whether to use out-of-bag samples to estimate the generalization accuracy.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_scores = cross_val_score(rf_model, X_tree, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(rf_scores)\n",
    "\n",
    "\"\"\"\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'criterion': ['mse', 'mae'],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10, 15],\n",
    "    'min_weight_fraction_leaf': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "    'max_leaf_nodes': [None, 10, 20, 30, 40, 50],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'bootstrap': [True, False],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf_model, param_grid_rf, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_rf.fit(X_tree, y_train)\n",
    "test_score_rf = grid_search_rf.best_estimator_.score(X_tree, y_train)\n",
    "\n",
    "print(f\"Best n_estimators value: {grid_search_rf.best_params_['n_estimators']}\")\n",
    "print(f\"Best max_depth value: {grid_search_rf.best_params_['max_depth']}\")\n",
    "print(f\"Best min_samples_split value: {grid_search_rf.best_params_['min_samples_split']}\")\n",
    "print(f\"Best min_samples_leaf value: {grid_search_rf.best_params_['min_samples_leaf']}\")\n",
    "print(f\"Best max_features value: {grid_search_rf.best_params_['max_features']}\")\n",
    "print(f\"Best bootstrap value: {grid_search_rf.best_params_['bootstrap']}\")\n",
    "print(f\"Best negative mean squared error: {grid_search_rf.best_score_}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Gradient Boosted Regression\n",
    "Gradient Boosted Regression is an ensemble machine learning technique that builds a strong predictive model by combining a set of weak models (usually decision trees) in a sequential manner. At each iteration, a new weak model is added to the ensemble, and the existing models are updated to minimize the prediction errors.\n",
    "\n",
    "Relevant Parameters:\n",
    "\n",
    "**n_estimators**: The number of boosting stages (trees) to be constructed. Increasing the number of trees can improve the model's performance, but it can also lead to overfitting if too many trees are added.\n",
    "\n",
    "**learning_rate**: The rate at which the contribution of each tree is shrunk. A smaller learning rate requires more trees in the ensemble but often leads to a more accurate model.\n",
    "\n",
    "**max_depth**: The maximum depth of the individual regression estimators (trees). Deeper trees can capture more complex patterns in the data, but they can also lead to overfitting. Shallower trees are more likely to underfit the data.\n",
    "\n",
    "**min_samples_split**: The minimum number of samples required to split an internal node in the decision tree. Higher values help prevent overfitting by avoiding splits on small subsets of the data.\n",
    "\n",
    "**min_samples_leaf**: The minimum number of samples required to be at a leaf node in the decision tree. Higher values help prevent overfitting by avoiding splits that result in small leaf nodes.\n",
    "\n",
    "**max_features**: The number of features to consider when looking for the best split in the decision tree. Reducing the number of features can help prevent overfitting and reduce the computation time.\n",
    "**subsample**: The fraction of samples to be used for fitting the individual base learners (trees). Smaller values introduce randomness and can help prevent overfitting. A value of 1.0 means all samples are used for each tree.\n",
    "\n",
    "**loss**: The loss function to be optimized. Options include 'ls' (least squares), 'lad' (least absolute deviation), 'huber' (a combination of least squares and least absolute deviation), and 'quantile' (quantile regression). The choice of loss function depends on the specific problem and the desired robustness to outliers.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific regression problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "gb_scores = cross_val_score(gb_model, X_tree, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(gb_scores)\n",
    "\n",
    "\"\"\"\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2],\n",
    "    'subsample': [0.5, 0.75, 1.0],\n",
    "    'criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10, 15],\n",
    "    'min_weight_fraction_leaf': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'max_depth': [3, 10, 20, 30, 40, 50],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "    'max_leaf_nodes': [None, 10, 20, 30, 40, 50],\n",
    "    'alpha': [0.1, 0.5, 0.9],\n",
    "    'validation_fraction': [0.1, 0.2, 0.3],\n",
    "    'n_iter_no_change': [None, 5, 10, 15],\n",
    "    'tol': [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "}\n",
    "\n",
    "grid_search_gb = GridSearchCV(gb_model, param_grid_gb, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_gb.fit(X_tree, y_train)\n",
    "test_score_gb = grid_search_gb.best_estimator_.score(X_tree, y_train)\n",
    "\n",
    "print(f\"Best n_estimators value: {grid_search_gb.best_params_['n_estimators']}\")\n",
    "print(f\"Best learning_rate value: {grid_search_gb.best_params_['learning_rate']}\")\n",
    "print(f\"Best max_depth value: {grid_search_gb.best_params_['max_depth']}\")\n",
    "print(f\"Best min_samples_split value: {grid_search_gb.best_params_['min_samples_split']}\")\n",
    "print(f\"Best min_samples_leaf value: {grid_search_gb.best_params_['min_samples_leaf']}\")\n",
    "print(f\"Best negative mean squared error: {grid_search_gb.best_score_}\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - XGBoost\n",
    "XGBoost (eXtreme Gradient Boosting) is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework, offering several regularization techniques to prevent overfitting.\n",
    "\n",
    "Relevant Parameters:\n",
    "\n",
    "**learning_rate**: Boosting learning rate. Controls the contribution of each tree in the ensemble. Lower learning rates lead to more robust models but require more trees (n_estimators).\n",
    "\n",
    "**n_estimators**: Number of boosting rounds to be run. Larger values result in more complex models but can increase the risk of overfitting.\n",
    "\n",
    "**max_depth**: Maximum tree depth for base learners. Controls the depth of each individual tree in the ensemble. Deeper trees can capture more complex patterns, but may also overfit the data.\n",
    "\n",
    "**min_child_weight**: Minimum sum of instance weight (hessian) needed in a child. Defines the minimum number of instances required for a node to be split.\n",
    "\n",
    "**gamma**: Minimum loss reduction required to make a further partition on a leaf node of the tree. Controls the complexity of the tree by reducing the number of splits made.\n",
    "\n",
    "**subsample**: Subsample ratio of the training instances. Setting it to a value less than 1.0 can help prevent overfitting.\n",
    "\n",
    "**colsample_bytree**: Subsample ratio of columns when constructing each tree. A smaller value can reduce overfitting and speed up the training process.\n",
    "\n",
    "**colsample_bylevel**: Subsample ratio of columns for each level. Specifies the fraction of features to choose for each level in the tree building process.\n",
    "\n",
    "**colsample_bynode**: Subsample ratio of columns for each split. Specifies the fraction of features to choose for each split in the tree building process.\n",
    "\n",
    "**reg_alpha**: L1 regularization term on weights. Controls the sparsity of feature weights, effectively performing feature selection.\n",
    "\n",
    "**reg_lambda**: L2 regularization term on weights. Smoothens the weights, preventing extreme values and reducing the risk of overfitting.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "\n",
    "# XGBoost\n",
    "\n",
    "xgb_model = XGBRegressor(tree_method='gpu_hist')\n",
    "xgb_scores = cross_val_score(xgb_model, X_tree, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(xgb_scores)\n",
    "\n",
    "\"\"\"\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 9, 12],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'subsample': [0.5, 0.75, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.75, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0],\n",
    "    'scale_pos_weight': [1, 2, 3, 4],\n",
    "    'booster': ['gbtree', 'gblinear', 'dart'],\n",
    "    'tree_method': ['auto', 'exact', 'approx', 'hist', 'gpu_hist'],\n",
    "    'grow_policy': ['depthwise', 'lossguide'],\n",
    "    'max_leaves': [0, 10, 20, 30, 40, 50],\n",
    "    'sampling_method': ['uniform', 'gradient_based']\n",
    "}\n",
    "\n",
    "random_search_xgb = RandomizedSearchCV(xgb_model, param_grid_xgb, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=1, n_iter=50)\n",
    "random_search_xgb.fit(X_tree, y_train)\n",
    "test_score_xgb = random_search_xgb.best_estimator_.score(X_tree, y_train)\n",
    "\n",
    "print(f\"Best n_estimators value: {random_search_xgb.best_params_['n_estimators']}\")\n",
    "print(f\"Best learning_rate value: {random_search_xgb.best_params_['learning_rate']}\")\n",
    "print(f\"Best max_depth value: {random_search_xgb.best_params_['max_depth']}\")\n",
    "print(f\"Best min_child_weight value: {random_search_xgb.best_params_['min_child_weight']}\")\n",
    "print(f\"Best subsample value: {random_search_xgb.best_params_['subsample']}\")\n",
    "print(f\"Best colsample_bytree value: {random_search_xgb.best_params_['colsample_bytree']}\")\n",
    "print(f\"Best gamma value: {random_search_xgb.best_params_['gamma']}\")\n",
    "print(f\"Best reg_alpha value: {random_search_xgb.best_params_['reg_alpha']}\")\n",
    "print(f\"Best reg_lambda value: {random_search_xgb.best_params_['reg_lambda']}\")\n",
    "print(f\"Best negative mean squared error: {random_search_xgb.best_score_}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########Data Impuation & Scaling (KNN, Clustering, ANN) ###############\n",
    "knn_pipe = Pipeline([\n",
    "    ('impute_trans', impute_preprocessor),\n",
    "    ('categorical_trans', category_preprocessor),\n",
    "    ('feat_scaling', scaling_preprocessor),\n",
    "    ('onehot', onehot_preprocessor)\n",
    "])\n",
    "\n",
    "X_knn = knn_pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hyperparameter Tuning - KNN\n",
    "k-Nearest Neighbors (KNN) is a simple, yet powerful, non-parametric supervised learning algorithm used for classification and regression. It assigns a new instance to the majority class or computes the mean (for regression tasks) of its k nearest neighbors in the feature space.\n",
    "\n",
    "**Relevant Parameters**:\n",
    "\n",
    "**n_neighbors**: Number of neighbors to use for the query. This is the main hyperparameter controlling the complexity of the KNN model. Larger values of k lead to smoother decision boundaries, while smaller values can capture more complex patterns but may overfit the data.\n",
    "\n",
    "**weights**: Weight function used in prediction. There are two options: 'uniform' (all points in each neighborhood are weighted equally) and 'distance' (assign weights proportional to the inverse of the distance from the query point). Using 'distance' can help reduce the impact of noise in the data.\n",
    "\n",
    "**algorithm**: Algorithm used to compute the nearest neighbors. Options include 'auto', 'ball_tree', 'kd_tree', and 'brute'. 'auto' will attempt to decide the most appropriate algorithm based on the values passed to fit() method. Choose the algorithm that best suits your data and computational requirements.\n",
    "\n",
    "**leaf_size**: Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.\n",
    "\n",
    "**p**: Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and for p = 2, it's equivalent to using euclidean_distance (l2). A larger value of p can help capture the specific geometry of your feature space.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# KNN\n",
    "knn_model = KNeighborsRegressor()\n",
    "knn_scores = cross_val_score(knn_model, X_knn, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(knn_scores)\n",
    "\n",
    "\"\"\"\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "grid_search_knn = GridSearchCV(knn_model, param_grid_knn, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_knn.fit(X_knn, y_train)\n",
    "test_score_knn = grid_search_knn.best_estimator_.score(X_knn, y_train)\n",
    "\n",
    "print(f\"Best n_neighbors value: {grid_search_knn.best_params_['n_neighbors']}\")\n",
    "print(f\"Best weights value: {grid_search_knn.best_params_['weights']}\")\n",
    "print(f\"Best metric value: {grid_search_knn.best_params_['metric']}\")\n",
    "print(f\"Best negative mean squared error: {grid_search_knn.best_score_}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - MLP Regression\n",
    "Multilayer Perceptron (MLP) Regression is a type of feedforward artificial neural network that can be used for regression tasks. MLPs consist of an input layer, one or more hidden layers, and an output layer. They learn to map input features to output values by adjusting the weights and biases of the network through a process called backpropagation.\n",
    "\n",
    "**Relevant Parameters**:\n",
    "\n",
    "**hidden_layer_sizes**: The number of hidden layers and the number of neurons in each hidden layer. This is specified as a tuple, e.g., (100,) means one hidden layer with 100 neurons, and (50, 50) means two hidden layers, each with 50 neurons. The choice of hidden layers and neurons can greatly impact the model's complexity and generalization performance.\n",
    "\n",
    "**activation**: The activation function used for the hidden layers. Options include 'identity', 'logistic' (sigmoid), 'tanh', and 'relu' (rectified linear unit). The choice of activation function depends on the specific problem and the desired non-linearity in the model.\n",
    "\n",
    "**solver**: The optimization algorithm used for weight and bias updates. Options include 'lbfgs', 'sgd' (stochastic gradient descent), and 'adam'. The choice of solver depends on the size and structure of the data and the desired computational efficiency.\n",
    "\n",
    "**alpha**: L2 regularization parameter (also called the weight decay). A positive float value that adds a penalty term to the loss function, helping to prevent overfitting by reducing the magnitude of the weights.\n",
    "\n",
    "**batch_size**: The size of the minibatches used for stochastic optimization. A smaller batch size introduces more randomness into the optimization process, which can help prevent overfitting and escape local minima.\n",
    "\n",
    "**learning_rate**: The learning rate schedule for weight updates. Options include 'constant', 'invscaling', and 'adaptive'. The choice of learning rate schedule depends on the solver and the desired convergence behavior.\n",
    "\n",
    "**learning_rate_init**: The initial learning rate used for weight updates. A smaller learning rate requires more iterations to converge but can result in a more accurate model.\n",
    "\n",
    "**max_iter**: The maximum number of iterations for the solver. Increasing the number of iterations can improve the model's performance, but it can also lead to overfitting if too many iterations are used.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific regression problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# ANN\n",
    "ann_model = MLPRegressor(random_state=42)\n",
    "ann_scores = cross_val_score(ann_model, X_tree, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(ann_scores)\n",
    "\n",
    "\"\"\"param_grid_ann = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive']\n",
    "}\n",
    "\n",
    "grid_search_ann = GridSearchCV(ann_model, param_grid_ann, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_ann.fit(X_tree, y_train)\n",
    "test_score_ann = grid_search_ann.best_estimator_.score(X_tree, y_train)\n",
    "\n",
    "print(f\"Best hidden_layer_sizes value: {grid_search_ann.best_params_['hidden_layer_sizes']}\")\n",
    "print(f\"Best activation value: {grid_search_ann.best_params_['activation']}\")\n",
    "print(f\"Best solver value: {grid_search_ann.best_params_['solver']}\")\n",
    "print(f\"Best alpha value: {grid_search_ann.best_params_['alpha']}\")\n",
    "print(f\"Best learning_rate value: {grid_search_ann.best_params_['learning_rate']}\")\n",
    "print(f\"Best negative mean squared error: {grid_search_ann.best_score_}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_reg = reg_pipe.transform(X_test)\n",
    "x_test_rr = rr_pipe.transform(X_test)\n",
    "x_test_Tree = tree_pipe.transform(X_test)\n",
    "x_test_knn = knn_pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_lr.fit(X_Reg_fin, y_reg_fin)\n",
    "lasso_model.fit(X_rr_fin, y_rr_fin) \n",
    "ridge_model.fit(X_rr_fin, y_rr_fin) \n",
    "en_model.fit(X_rr_fin, y_rr_fin) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model.fit(X_rr_fin, y_rr_fin) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model.fit(X_tree, y_train)\n",
    "rf_model.fit(X_tree, y_train)\n",
    "gb_model.fit(X_tree, y_train)\n",
    "xgb_model.fit(X_tree, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model.fit(X_knn, y_train)\n",
    "ann_model.fit(X_knn, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Linear Regression\n",
    "y_pred_lr = model_lr.predict(x_test_reg.values)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "print(f\"Linear Regression neg MSE: {-mse_lr}\")\n",
    "\n",
    "# Lasso\n",
    "y_pred_lasso = lasso_model.predict(x_test_rr.values)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "print(f\"Lasso neg MSE: {-mse_lasso}\")\n",
    "\n",
    "# Ridge\n",
    "y_pred_ridge = ridge_model.predict(x_test_rr.values)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "print(f\"Ridge neg MSE: {-mse_ridge}\")\n",
    "\n",
    "# Elastic Net\n",
    "y_pred_en = en_model.predict(x_test_rr.values)\n",
    "mse_en = mean_squared_error(y_test, y_pred_en)\n",
    "print(f\"Elastic Net neg MSE: {-mse_en}\")\n",
    "\n",
    "# SVM\n",
    "y_pred_svm = svm_model.predict(x_test_rr.values)\n",
    "mse_svm = mean_squared_error(y_test, y_pred_svm)\n",
    "print(f\"SVM neg MSE: {-mse_svm}\")\n",
    "\n",
    "# Decision Tree\n",
    "y_pred_dt = dt_model.predict(x_test_Tree.values)\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "print(f\"Decision Tree neg MSE: {-mse_dt}\")\n",
    "\n",
    "# Random Forest\n",
    "y_pred_rf = rf_model.predict(x_test_Tree.values)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(f\"Random Forest neg MSE: {-mse_rf}\")\n",
    "\n",
    "# Gradient Boosting\n",
    "y_pred_gb = gb_model.predict(x_test_Tree.values)\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "print(f\"Gradient Boosting neg MSE: {-mse_gb}\")\n",
    "\n",
    "# XGBoost\n",
    "y_pred_xgb = xgb_model.predict(x_test_Tree.values)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost neg MSE: {-mse_xgb}\")\n",
    "\n",
    "# KNN\n",
    "y_pred_knn = knn_model.predict(x_test_knn.values)\n",
    "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
    "print(f\"KNN neg MSE: {-mse_knn}\")\n",
    "\n",
    "# ANN\n",
    "y_pred_ann = ann_model.predict(x_test_knn.values)\n",
    "mse_ann = mean_squared_error(y_test, y_pred_ann)\n",
    "print(f\"ANN neg MSE: {-mse_ann}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
